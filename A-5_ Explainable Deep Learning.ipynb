{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGGdFPX1GbIxmRLd8J3BVa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment: Explainable Deep Learning\n","\n","# Class: AIPI 590\n","\n","# Author: Ramil Mammadov\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QZiK9Hk8XBQPF7JkYlpxYWv0i9gIadDk?usp=sharing)\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"hf1l0iEL0-55"}},{"cell_type":"markdown","source":["## Setup & imports\n","This cell installs and imports everything the notebook needs (PyTorch, TorchVision, Pillow, Matplotlib, and pytorch-grad-cam). It also includes a small fallback to install pytorch-grad-cam from GitHub if PyPI is flaky. Finally, it detects whether a GPU is available and sets device accordingly. Nothing in this step changes the model; it just guarantees a clean, reproducible environment."],"metadata":{"id":"PDapY-nRRiq1"}},{"cell_type":"code","source":["!pip -q install -U \"jedi>=0.18.0\"\n","!pip -q install -U pip setuptools wheel\n","!pip -q install -U torch torchvision opencv-python matplotlib Pillow\n","\n","# pytorch-grad-cam\n","import sys, subprocess\n","try:\n","    import pytorch_grad_cam\n","except Exception:\n","    try:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n","                               \"pytorch-grad-cam>=1.4.8\", \"--no-cache-dir\"])\n","    except Exception:\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n","                               \"git+https://github.com/jacobgil/pytorch-grad-cam\"])\n","\n","# Importing libraries\n","import os, random, math, warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM, EigenCAM, XGradCAM, LayerCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NAIBHpHQGlm","executionInfo":{"status":"ok","timestamp":1759718737745,"user_tz":240,"elapsed":20430,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}},"outputId":"66a010fa-6920-4070-b5a3-fbd6fb1b1c62"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"markdown","source":["## Oxford-IIIT Pet loader (adapter)\n","\n","We download the Oxford-IIIT Pet dataset and wrap it with a tiny adapter so every sample returns a consistent (PIL_image, class_name) pair, regardless of TorchVision version. This avoids version-specific target formats and keeps later code simple. We also pick a small subset (≥5 images) to keep visualization snappy."],"metadata":{"id":"4Co_r4TmR1YB"}},{"cell_type":"code","source":["root = \"./data\"\n","pets = torchvision.datasets.OxfordIIITPet(root=root, download=True)\n","\n","class PetsAdapter:\n","    \"\"\"ds[idx] -> (PIL.Image, human_readable_class_name) across torchvision versions.\"\"\"\n","    def __init__(self, ds):\n","        self.ds = ds\n","        assert hasattr(self.ds, \"classes\"), \"OxfordIIITPet should expose .classes list\"\n","        self.categories = list(self.ds.classes)\n","        _, tgt0 = self.ds[0]\n","        if isinstance(tgt0, (tuple, list)):   self._target_kind = \"tuple\"\n","        elif isinstance(tgt0, dict):          self._target_kind = \"dict\"\n","        else:                                  self._target_kind = \"int\"\n","\n","    def __len__(self):  return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        img, tgt = self.ds[idx]\n","        if self._target_kind == \"tuple\":  label_index = int(tgt[0])\n","        elif self._target_kind == \"dict\": label_index = int(tgt.get(\"category\", tgt.get(\"label\", 0)))\n","        else:                             label_index = int(tgt)\n","        cname = self.ds.classes[label_index]\n","        return img, cname\n","\n","ds = PetsAdapter(pets)\n","\n","# Using a small subset for visualization\n","random.seed(7)\n","picked_indices = [i for i in range(min(12, len(ds)))]\n","print(\"Total picked:\", len(picked_indices))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VK6Px-agQLnK","executionInfo":{"status":"ok","timestamp":1759718742070,"user_tz":240,"elapsed":193,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}},"outputId":"63be17c6-8717-4b20-867e-576517b495f8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Total picked: 12\n"]}]},{"cell_type":"markdown","source":["## Pretrained ResNet-50 + transforms\n","\n","Here we load an ImageNet-pretrained ResNet-50 (no training) and grab its canonical preprocessing pipeline (resize, crop, normalize). We also keep the 1,000 ImageNet class names so we can display human-readable predictions."],"metadata":{"id":"uhzifBzASIHX"}},{"cell_type":"code","source":["weights = ResNet50_Weights.DEFAULT\n","model = resnet50(weights=weights).to(device).eval()\n","\n","# Preprocessing pipeline recommended by the weights\n","preprocess = weights.transforms()\n","\n","# Human-readable ImageNet classes\n","imagenet_labels = weights.meta[\"categories\"]\n","print(\"Model & transforms ready.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gELKQeiQSWU","executionInfo":{"status":"ok","timestamp":1759718745655,"user_tz":240,"elapsed":1589,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}},"outputId":"ee134042-0329-4b3c-f062-e1f7009ec382"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model & transforms ready.\n"]}]},{"cell_type":"markdown","source":["## Helper functions\n","\n","These utilities do the unglamorous but essential work. We convert PIL → model tensors, run a forward pass to get logits/probabilities, and convert normalized tensors back to HWC 8-bit images for overlays (fixing the common CHW/HWC plotting error). We also pick the last conv block (layer4[-1]) as the CAM target and provide a small factory to build different CAM variants without repeating code."],"metadata":{"id":"C3r_BWCRSWvX"}},{"cell_type":"code","source":["IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","def pil_to_model_tensor(pil_img):\n","    \"\"\"PIL -> normalized tensor (1,3,224,224) on device.\"\"\"\n","    return preprocess(pil_img).unsqueeze(0).to(device)\n","\n","@torch.inference_mode()\n","def predict_logits(t1x3x224):\n","    logits = model(t1x3x224)\n","    probs = F.softmax(logits, dim=1)\n","    return logits, probs\n","\n","def topk_from_probs(probs, k=5):\n","    topk_probs, topk_idx = torch.topk(probs, k, dim=1)\n","    topk_probs = [float(p) for p in topk_probs.squeeze(0).tolist()]\n","    topk_idx = topk_idx.squeeze(0).tolist()\n","    topk_labels = [imagenet_labels[i] for i in topk_idx]\n","    return list(zip(topk_labels, topk_probs))\n","\n","def tensor_to_showable_rgb(t):\n","    \"\"\"(1,3,224,224) normalized -> (224,224,3) uint8 for display.\"\"\"\n","    x = t.detach().squeeze(0).cpu().numpy()\n","    x = np.transpose(x, (1, 2, 0))\n","    x = (x * IMAGENET_STD) + IMAGENET_MEAN\n","    x = np.clip(x, 0.0, 1.0)\n","    return (x * 255.0).round().astype(np.uint8)\n","\n","def show_image(img, title=None):\n","    plt.imshow(img); plt.axis(\"off\")\n","    if title: plt.title(title)\n","\n","def pick_target_layers_resnet(m):\n","    return [m.layer4[-1]]\n","\n","def build_cam(cam_name, model, target_layers):\n","    cam_name = cam_name.lower()\n","    if cam_name == \"gradcam\":     return GradCAM(model=model, target_layers=target_layers)\n","    if cam_name == \"gradcam++\":   return GradCAMPlusPlus(model=model, target_layers=target_layers)\n","    if cam_name == \"eigen-cam\":   return EigenCAM(model=model, target_layers=target_layers)\n","    if cam_name == \"score-cam\":   return ScoreCAM(model=model, target_layers=target_layers)\n","    if cam_name == \"xgrad-cam\":   return XGradCAM(model=model, target_layers=target_layers)\n","    if cam_name == \"layer-cam\":   return LayerCAM(model=model, target_layers=target_layers)\n","    raise ValueError(f\"Unknown CAM method: {cam_name}\")\n"],"metadata":{"id":"0dBvgAwuQVUJ","executionInfo":{"status":"ok","timestamp":1759718746973,"user_tz":240,"elapsed":30,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Baseline CAMs (Grad-CAM, Grad-CAM++, Eigen-CAM, + Score-/XGrad-CAM)\n","\n","For each chosen image we: (1) run the model to get a Top-1 class, (2) compute CAM heatmaps for multiple methods targeting that class, and (3) overlay the heatmaps on the image. We plot one figure per image to avoid subplot overflows and save the overlays to disk for our report."],"metadata":{"id":"nFfOKBJVSkvr"}},{"cell_type":"code","source":["base_methods = [\"GradCAM\", \"GradCAM++\", \"Eigen-CAM\"]\n","extra_method = \"Score-CAM\" if device.type == \"cuda\" else \"XGrad-CAM\"\n","methods = base_methods + [extra_method]\n","print(\"Methods:\", methods)\n","\n","target_layers = pick_target_layers_resnet(model)\n","os.makedirs(\"outputs\", exist_ok=True)\n","\n","all_results = []\n","\n","for idx in picked_indices:\n","    pil_img, pet_class = ds[idx]\n","\n","    # Forward pass\n","    inp = pil_to_model_tensor(pil_img)\n","    logits, probs = predict_logits(inp)\n","    top1_idx = int(torch.argmax(probs, dim=1).item())\n","    top1_label = imagenet_labels[top1_idx]\n","    top5 = topk_from_probs(probs, k=5)\n","\n","    # Base image for overlays\n","    show_np_uint8 = tensor_to_showable_rgb(inp)\n","    base_float = show_np_uint8.astype(np.float32) / 255.0\n","\n","    # One figure per image\n","    ncols = 1 + len(methods)\n","    plt.figure(figsize=(4*ncols, 3))\n","    plt.subplot(1, ncols, 1)\n","    show_image(show_np_uint8, title=f\"Oxford-Pet: {pet_class}\\nPred: {top1_label}\")\n","\n","    targets = [ClassifierOutputTarget(top1_idx)]\n","    heatmaps = {}\n","\n","    for j, mname in enumerate(methods, start=2):\n","        with build_cam(mname, model, target_layers) as cam:\n","            grayscale_cam = cam(input_tensor=inp, targets=targets)[0]\n","        heatmaps[mname] = grayscale_cam\n","\n","        overlay = show_cam_on_image(base_float, grayscale_cam, use_rgb=True)\n","        plt.subplot(1, ncols, j); show_image(overlay, title=mname)\n","\n","        Image.fromarray(overlay).save(f\"outputs/pet_{idx}_{pet_class}_{mname.replace(' ','_')}.jpg\")\n","\n","    plt.tight_layout(); plt.show()\n","\n","    all_results.append({\n","        \"pet_class\": pet_class,\n","        \"imagenet_top1\": top1_label,\n","        \"top5\": top5,\n","        \"heatmaps\": heatmaps,\n","    })\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"18nRbyem4M2aNXEosuy_Y-b7iMtumzF0t"},"id":"k8kAwVjEQYU_","executionInfo":{"status":"ok","timestamp":1759718801688,"user_tz":240,"elapsed":52243,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}},"outputId":"096fbecd-7d4b-46f1-f4c6-b82e0b3c0b47"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Logit masking (restrict to cat/dog classes)\n","\n","Because our model is trained on ImageNet, it can pick classes like “plastic bag.” Logit masking keeps the backbone untouched but re-scores only among ImageNet classes containing “cat” or “dog.” That reduces label-space mismatch and steers predictions (and thus explanations) toward pet-relevant classes without any training."],"metadata":{"id":"G3B0bO5LStAO"}},{"cell_type":"code","source":["catdog_indices = [i for i, n in enumerate(imagenet_labels)\n","                  if (\"cat\" in n.lower()) or (\"dog\" in n.lower())]\n","\n","def predict_with_label_mask(inp, allowed_indices=catdog_indices, topk=5):\n","    logits = model(inp)\n","    mask = torch.full_like(logits, -1e9)\n","    mask[:, allowed_indices] = 0.0\n","    masked_logits = logits + mask\n","    local = torch.softmax(masked_logits[:, allowed_indices], dim=1)\n","    k = min(topk, len(allowed_indices))\n","    probs_k, idx_local = torch.topk(local, k=k, dim=1)\n","    idx_local = idx_local.squeeze(0).tolist()\n","    probs_k = probs_k.squeeze(0).tolist()\n","    idx_global = [allowed_indices[i] for i in idx_local]\n","    labels = [imagenet_labels[i] for i in idx_global]\n","    top1_global_idx = idx_global[0]\n","    return list(zip(labels, probs_k)), top1_global_idx\n"],"metadata":{"id":"fucXkvqpQw5P","executionInfo":{"status":"ok","timestamp":1759718815054,"user_tz":240,"elapsed":4,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Cat/Dog detector crop (OPTIONAL)\n","\n","We run a lightweight COCO Faster R-CNN to find cat/dog boxes, then crop the image around the best detection (with a small margin). This reduces background dominance (e.g., bags, blankets) and produces more object-centric inputs before classification. If no cat/dog is detected confidently, we safely fall back to the original image."],"metadata":{"id":"N2IO-LIvS3Gl"}},{"cell_type":"code","source":["from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n","\n","det_weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","detector = fasterrcnn_resnet50_fpn(weights=det_weights).to(device).eval()\n","det_tfms = det_weights.transforms()\n","coco_names = det_weights.meta[\"categories\"]\n","cat_id = coco_names.index(\"cat\")\n","dog_id = coco_names.index(\"dog\")\n","\n","def crop_catdog(pil_img, score_thresh=0.6, margin=0.10):\n","    \"\"\"Crop around the best cat/dog box; fallback to original image.\"\"\"\n","    img_t = det_tfms(pil_img).to(device)\n","    with torch.no_grad():\n","        out = detector([img_t])[0]\n","    boxes = out[\"boxes\"].detach().cpu().numpy()\n","    labels = out[\"labels\"].detach().cpu().numpy()\n","    scores = out[\"scores\"].detach().cpu().numpy()\n","\n","    keep = [i for i,(lab,s) in enumerate(zip(labels, scores))\n","            if (lab in (cat_id, dog_id)) and (s >= score_thresh)]\n","    if not keep:\n","        return pil_img\n","\n","    i = int(np.argmax(scores[keep]))\n","    x1, y1, x2, y2 = boxes[keep[i]]\n","    W, H = pil_img.size\n","    w, h = x2 - x1, y2 - y1\n","    x1 = max(0, int(x1 - margin * w)); y1 = max(0, int(y1 - margin * h))\n","    x2 = min(W, int(x2 + margin * w)); y2 = min(H, int(y2 + margin * h))\n","    return pil_img.crop((x1, y1, x2, y2))\n"],"metadata":{"id":"R5dQGOD4Q0T0","executionInfo":{"status":"ok","timestamp":1759718818524,"user_tz":240,"elapsed":1074,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Audit loop (Original vs Masked vs Cropped+Masked with Grad-CAM++ & Eigen-CAM) (OPTIONAL)\n","\n","This is our “sanity-check dashboard.” For a few images, we show: the unmodified prediction and maps; the masked prediction/maps (cat-only); and the cropped+masked prediction/maps. We use Grad-CAM++ (better for multiple salient parts/instances) and Eigen-CAM (good object-level coverage) to compare how each mitigation shifts attention away from backgrounds and toward the animal. It’s a lightweight, repeatable QA step we can run anytime to catch shortcuts and multi-instance issues."],"metadata":{"id":"cwweQKhPTDzq"}},{"cell_type":"code","source":["def run_cam(model, inp, class_idx, method=\"++\"):\n","    target_layers = pick_target_layers_resnet(model)\n","    Target = ClassifierOutputTarget(class_idx)\n","    cam_cls = GradCAMPlusPlus if method == \"++\" else EigenCAM\n","    with cam_cls(model=model, target_layers=target_layers) as cam:\n","        return cam(input_tensor=inp, targets=[Target])[0]  # HxW\n","\n","def audit_image(idx, title_prefix=\"\"):\n","    pil_img, pet_class = ds[idx]\n","\n","    # Original (unmasked)\n","    x = pil_to_model_tensor(pil_img)\n","    _, p = predict_logits(x)\n","    top1_full = int(torch.argmax(p,1))\n","    label_full = imagenet_labels[top1_full]\n","\n","    # Masked to cat\n","    top5_masked, top1_masked = predict_with_label_mask(x)\n","    label_masked = imagenet_labels[top1_masked]\n","\n","    # Cropped + masked\n","    crop = crop_catdog(pil_img)\n","    x_crop = pil_to_model_tensor(crop)\n","    top5_crop_masked, top1_crop_masked = predict_with_label_mask(x_crop)\n","    label_crop_masked = imagenet_labels[top1_crop_masked]\n","\n","    base_orig = tensor_to_showable_rgb(x).astype(np.float32)/255.0\n","    base_crop = tensor_to_showable_rgb(x_crop).astype(np.float32)/255.0\n","\n","    campp_orig = run_cam(model, x,      top1_masked, method=\"++\")\n","    eigen_orig = run_cam(model, x,      top1_masked, method=\"eigen\")\n","    campp_crop = run_cam(model, x_crop, top1_crop_masked, method=\"++\")\n","    eigen_crop = run_cam(model, x_crop, top1_crop_masked, method=\"eigen\")\n","\n","    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n","\n","    axes[0,0].imshow(tensor_to_showable_rgb(x)); axes[0,0].axis(\"off\")\n","    axes[0,0].set_title(f\"{title_prefix}Orig\\nPred: {label_full}\")\n","\n","    axes[1,0].imshow(crop); axes[1,0].axis(\"off\")\n","    axes[1,0].set_title(f\"Crop\\nPred(masked): {label_crop_masked}\")\n","\n","    axes[0,1].imshow(show_cam_on_image(base_orig, campp_orig, use_rgb=True)); axes[0,1].axis(\"off\")\n","    axes[0,1].set_title(\"Grad-CAM++ (orig, masked target)\")\n","\n","    axes[1,1].imshow(show_cam_on_image(base_crop, campp_crop, use_rgb=True)); axes[1,1].axis(\"off\")\n","    axes[1,1].set_title(\"Grad-CAM++ (crop, masked target)\")\n","\n","    axes[0,2].imshow(show_cam_on_image(base_orig, eigen_orig, use_rgb=True)); axes[0,2].axis(\"off\")\n","    axes[0,2].set_title(\"Eigen-CAM (orig, masked target)\")\n","\n","    axes[1,2].imshow(show_cam_on_image(base_crop, eigen_crop, use_rgb=True)); axes[1,2].axis(\"off\")\n","    axes[1,2].set_title(\"Eigen-CAM (crop, masked target)\")\n","\n","    plt.tight_layout(); plt.show()\n","\n","# Run audit on a few images\n","for k in [0, 1, 2]:\n","    audit_image(picked_indices[k], title_prefix=f\"Oxford-Pet: {ds[picked_indices[k]][1]}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1EAcHmOs-M3begLjKCAyWJnsyv7hnSeBo"},"id":"9bTx3xNVQ3m4","executionInfo":{"status":"ok","timestamp":1759718864425,"user_tz":240,"elapsed":39832,"user":{"displayName":"Ramil Mammadov","userId":"09256801984926553900"}},"outputId":"64b50c9e-aec6-4d0c-871e-e806f20bd6c6"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Compare & contrast: Grad-CAM vs. its variants\n","\n","Across all Abyssinian images, the four methods agree on the face as the key region, but they differ in how they highlight it:\n","\n","- Grad-CAM concentrates on the most discriminative part (often a single eye + whisker pad). It’s sharp and part-centric. In the two-kitten scene it mostly locks onto the front cat and under-covers the second one.\n","\n","- Grad-CAM++ distributes relevance more evenly across multiple facial parts and across multiple instances when they’re present. In the twin-kitten image it lights both faces better than vanilla Grad-CAM. It’s a bit broader/softer but more complete.\n","\n","- Eigen-CAM produces a cohesive, object-level mask—a smooth oval over the entire head/upper torso with a bright core. It has the least background leakage when the class is correct, and it’s a great sanity check that the model “sees the whole head,” not just a tiny texture.\n","\n","- XGrad-CAM tends to mirror Grad-CAM’s choices but with slightly crisper transitions. Where Grad-CAM fires on eye/cheek, XGrad-CAM usually agrees, refining the boundary.\n","\n","In the “blue shopping bag” photo, before any mitigation the model’s Top-1 is “plastic bag,” and Grad-/XGrad-CAM heat sits on the bag rim and folds. After my label mask (cat/dog only) and detector crop, the Top-1 flips to “Egyptian cat” and both Grad-CAM++ and Eigen-CAM shift decisively to the eyes, muzzle, and ear—a clear demonstration that the mitigations remove a background shortcut and make all maps more semantically aligned."],"metadata":{"id":"qN_pstSDWiR6"}},{"cell_type":"markdown","source":["## Reflection\n","\n","### What visual cues the model attends to\n","The classifier relies foremost on facial structure: bright eye rims, the bridge of the nose, the whisker pad/muzzle, and ear triangles. Texture matters too—short, fine fur and high-contrast edges around the face. When the mouth is open, teeth and the dark oral cavity become dominant cues (we saw this in the “cougar” misprediction). In cluttered scenes, strong color/edge patterns near the face (e.g., the blue bag seam, blanket folds) can also attract attention.\n","\n","### Surprising or misleading behavior\n","Two issues are especially visible in our panels. First, context dominance: the “plastic bag” Top-1 arises because the bag occupies a large portion of the 224×224 crop and has highly discriminative textures for ImageNet classes; Grad-/XGrad-CAM faithfully highlight the bag, explaining the wrong prediction. Second, label-space mismatch: using an ImageNet head means outputs like “cougar” are possible; the maps (sensibly) light teeth/face for that class, but that’s not the label we care about in a pet-breed setting. There’s also a multi-instance bias: vanilla Grad-CAM often prefers one cat in the two-kitten shot, while Grad-CAM++ spreads attention more fairly. Watermarks and high-contrast logos sometimes draw weak activations, but in our examples they don’t dominate once the label is cat-restricted.\n","\n","### Why explainability matters here (pet perception)\n","If this system were used in a home robot, shelter intake, or monitoring context, actions should be driven by the animal, not by props or backgrounds. The CAMs make that verifiable. They also guide practical fixes: our logit masking and cat crop demonstrably pull focus from background objects to the cat’s face, and the audit grid with Grad-CAM++ and Eigen-CAM lets we re-check that behavior after any change. Finally, method choice itself is part of good practice: Grad-CAM++ is preferable in multi-pet scenes; Eigen-CAM gives a robust object-level sanity check; Grad-/XGrad-CAM are great for pinpointing the single most discriminative facial part. Together, these tools help we detect shortcuts early, collect better data (more diverse backgrounds, tighter crops), and build user trust that the model’s decisions come from the right pixels."],"metadata":{"id":"OZGveBR_WkpF"}},{"cell_type":"markdown","source":["## LLM\n","For this assignment, **OpenAI Chat GPT-5 Thinking** large language model was utilized to generate some of the Python code for the modeling and visualization steps. The tool was also used for correcting and adjusting markdown formatting."],"metadata":{"id":"aFCrZhkabFQA"}}]}